{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6baebe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 36328.pdf\n",
      "Processing: Code_of_criminal_procedure_1898.pdf\n",
      "Processing: Code_of_Criminal_Procedure_1898_incorporating_amendments_to_16_February_2017.pdf\n",
      "Processing: Domestic Violence (Prevention and Protection) Act, 2013 & Rules, 2016 (Amendments upto date).pdf\n",
      "Processing: Domestic-Violence-in-Pakistan.pdf\n",
      "Processing: domestic-violence-laws-and-their-legal-framework-for-women-in-pakistan-an-analysis.pdf\n",
      "Processing: ha.pdf\n",
      "MuPDF error: format error: No default Layer config\n",
      "\n",
      "Processing: Pakistan Penal Code.pdf\n",
      "Processing: Pakistan_Penal_Code_1860_incorporating_amendments_to_16_February_2017 (1).pdf\n",
      "Processing: Pakistan_Penal_Code_1860_incorporating_amendments_to_16_February_2017.pdf\n",
      "Processing: Protection-of-Women-Criminal-Laws-Amendment-Act-2006-Editors-Comment-PLR-Vol-III.pdf\n",
      "Processing: PUNJAB_PROTECTION_OF_WOMEN_AGAINST_VIOLENCE_ACT_2016 (1).pdf\n",
      "Processing: PUNJAB_PROTECTION_OF_WOMEN_AGAINST_VIOLENCE_ACT_2016.pdf\n",
      "Processing: the_code_of_criminal_procedure_1898.pdf\n",
      "Processing: Women protection against domestic violence bil 13pages.pdf\n",
      "\n",
      "Total chunks created: 4393\n",
      "\n",
      "--- Chunk 1 from 36328.pdf ---\n",
      "386 \n",
      "National Legislation on Violence against Women in Pakistan \n",
      "Journal of Peace, Development and Communication \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Volume 06, Issue 02, June 2022 \n",
      "pISSN: 2663-7898, eISSN: 2663-7901 \n",
      "Article DOI: https://doi.org/10.36968/JPDC-V06-I02-28 \n",
      "Homepage: https://pdfpk.net/pdf/ \n",
      "Email: se.jpdc@pdfpk.net \n",
      " \n",
      "Article: \n",
      "National Legislation on Violence against Women in Pakistan  \n",
      " \n",
      " \n",
      "Author(s): \n",
      " \n",
      " \n",
      " \n",
      "Dr. Rafia Naz Ali  \n",
      "Assistant Professor, Department of Shariah and Law, Islamia College Universi ...\n",
      "\n",
      "\n",
      "--- Chunk 2 from 36328.pdf ---\n",
      "ps://doi.org/doi.org/10.36968/JPDC-V06-I02-28 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Author(s) Note: \n",
      " \n",
      " \n",
      " \n",
      "Dr. Rafia Naz Ali is serving as an Assistant Professor at Department of Shariah and \n",
      "Law, Islamia College University Peshawar \n",
      "Corresponding Email: rafia@icp.edu.pk \n",
      "387 \n",
      "National Legislation on Violence against Women in Pakistan \n",
      " \n",
      "ABSTRACT \n",
      " \n",
      "Violence against women is a rampant crime and defilement of fundamental rights. Various \n",
      "international conventions and declarations bound the states to work for the eradication  ...\n",
      "\n",
      "\n",
      "--- Chunk 3 from 36328.pdf ---\n",
      "nd. But the legal framework lacks distinct or special legislation on violence. The \n",
      "legislature make several amendments in the existing criminal law to combat it. The aim of the \n",
      "paper is to describe national legislation in the context of rule of law as its theoretical framework \n",
      "on the subject. The study would analyse it qualitatively and present its findings as whether the \n",
      "existing laws are enough to prevent anti-women practices and protect their fundamental rights. \n",
      "Keywords: Violence agains ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "\n",
    "# Folder containing your PDFs\n",
    "pdf_folder = \"./KnowledgeBase\"\n",
    "\n",
    "# Define chunk size\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def split_text(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Load and process all PDFs\n",
    "all_chunks = []\n",
    "\n",
    "for file in os.listdir(pdf_folder):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        path = os.path.join(pdf_folder, file)\n",
    "        print(f\"Processing: {file}\")\n",
    "        text = extract_text_from_pdf(path)\n",
    "        chunks = split_text(text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            all_chunks.append({\n",
    "                \"file\": file,\n",
    "                \"chunk_number\": i + 1,\n",
    "                \"content\": chunk\n",
    "            })\n",
    "\n",
    "print(f\"\\nTotal chunks created: {len(all_chunks)}\")\n",
    "\n",
    "# Show sample chunks\n",
    "for i in range(min(3, len(all_chunks))):\n",
    "    print(f\"\\n--- Chunk {i+1} from {all_chunks[i]['file']} ---\")\n",
    "    print(all_chunks[i][\"content\"][:500], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3e99fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stored 4393 chunks into ChromaDB\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import uuid\n",
    "\n",
    "# Initialize Chroma client and collection\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"paklegal_laws\")\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Insert chunks\n",
    "for chunk in all_chunks:\n",
    "    content = chunk['content']\n",
    "    metadata = {\"file\": chunk['file'], \"chunk_id\": chunk['chunk_number']}\n",
    "    embedding = model.encode(content).tolist()\n",
    "    doc_id = str(uuid.uuid4())\n",
    "    \n",
    "    collection.add(\n",
    "        documents=[content],\n",
    "        embeddings=[embedding],\n",
    "        metadatas=[metadata],\n",
    "        ids=[doc_id]\n",
    "    )\n",
    "\n",
    "print(f\"✅ Stored {len(all_chunks)} chunks into ChromaDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6793f594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index created with 4393 documents.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data\n",
    "texts = [chunk['content'] for chunk in all_chunks]\n",
    "metadatas = [{\"file\": chunk['file'], \"chunk_id\": chunk['chunk_number']} for chunk in all_chunks]\n",
    "embeddings = model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "# Create FAISS index (dimension must match embedding size)\n",
    "dimension = embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(embeddings)\n",
    "\n",
    "print(f\"✅ FAISS index created with {len(texts)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04dbf84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index saved to faiss_index.idx\n"
     ]
    }
   ],
   "source": [
    "faiss.write_index(faiss_index, \"faiss_index.idx\")\n",
    "print(\"✅ FAISS index saved to faiss_index.idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62118244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All 10 results saved to TempOutput.txt\n"
     ]
    }
   ],
   "source": [
    "# query = \"man slapped his wife what is further legal proceedure.\"\n",
    "# query = \"What is the punishment for kidnapping a person in Pakistan?\"\n",
    "# query = \"What law protects women from domestic abuse in Punjab?\"\n",
    "query = \"What are the legal provisions and procedural steps under Pakistani law, including the Pakistan Penal Code and Domestic Violence Acts, in the case of a husband committing physical assault or criminal force against his wife?\"\n",
    "query_embedding = model.encode(query).tolist()\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=10\n",
    ")\n",
    "output_file = \"TempOutput.txt\"\n",
    "# Display top 3 results\n",
    "# for i, doc in enumerate(results[\"documents\"][0]):\n",
    "#     print(f\"\\n--- Result {i+1} ---\")\n",
    "#     print(doc[:1000], \"...\")\n",
    "#     print(\"Metadata:\", results[\"metadatas\"][0][i])\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, doc in enumerate(results[\"documents\"][0]):\n",
    "        metadata = results[\"metadatas\"][0][i]\n",
    "        f.write(f\"--- Result {i+1} ---\\n\")\n",
    "        f.write(f\"Source File: {metadata.get('file', 'N/A')}\\n\")\n",
    "        f.write(f\"Chunk ID: {metadata.get('chunk_id', 'N/A')}\\n\")\n",
    "        f.write(\"Content:\\n\")\n",
    "        f.write(doc.strip() + \"\\n\")\n",
    "        f.write(\"\\n\" + \"-\"*80 + \"\\n\\n\")\n",
    "\n",
    "print(f\"✅ All {len(results['documents'][0])} results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8bac8de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Top 3 FAISS results saved to TempOutput_FAISS.txt\n"
     ]
    }
   ],
   "source": [
    "# Sanity query test\n",
    "query1 = \"What law protects women from domestic abuse in Punjab?\"\n",
    "query_embedding = model.encode([query1], convert_to_numpy=True)\n",
    "\n",
    "# Perform search (top 3)\n",
    "D, I = faiss_index.search(query_embedding, k=10)\n",
    "\n",
    "# Save results to file\n",
    "output_file_faiss = \"TempOutput_FAISS.txt\"\n",
    "with open(output_file_faiss, \"w\", encoding=\"utf-8\") as f:\n",
    "    for rank, idx in enumerate(I[0]):\n",
    "        doc = texts[idx]\n",
    "        metadata = metadatas[idx]\n",
    "        f.write(f\"--- Result {rank+1} ---\\n\")\n",
    "        f.write(f\"Source File: {metadata.get('file', 'N/A')}\\n\")\n",
    "        f.write(f\"Chunk ID: {metadata.get('chunk_id', 'N/A')}\\n\")\n",
    "        f.write(\"Content:\\n\")\n",
    "        f.write(doc.strip() + \"\\n\")\n",
    "        f.write(\"\\n\" + \"-\"*80 + \"\\n\\n\")\n",
    "\n",
    "print(f\"✅ Top 3 FAISS results saved to {output_file_faiss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import uuid\n",
    "\n",
    "# Use a better embedding model (higher quality)\n",
    "model_name = \"all-mpnet-base-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# New Chroma collection for this model\n",
    "chroma_client = chromadb.Client()\n",
    "collection_mpnet = chroma_client.create_collection(name=\"paklegal_laws_mpnet\")\n",
    "\n",
    "# Embed and insert\n",
    "for chunk in all_chunks:\n",
    "    content = chunk['content']\n",
    "    metadata = {\"file\": chunk['file'], \"chunk_id\": chunk['chunk_number']}\n",
    "    embedding = model.encode(content).tolist()\n",
    "    doc_id = str(uuid.uuid4())\n",
    "\n",
    "    collection_mpnet.add(\n",
    "        documents=[content],\n",
    "        embeddings=[embedding],\n",
    "        metadatas=[metadata],\n",
    "        ids=[doc_id]\n",
    "    )\n",
    "\n",
    "print(f\"✅ Stored {len(all_chunks)} chunks into ChromaDB using {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d16af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"What law protects women from domestic abuse in Punjab?\"\n",
    "query_embedding2 = model.encode(query2).tolist()\n",
    "\n",
    "results2 = collection_mpnet.query(\n",
    "    query_embeddings=[query_embedding2],\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "# Print top results\n",
    "for i, doc in enumerate(results2[\"documents\"][0]):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(doc[:1000], \"...\")\n",
    "    print(\"Metadata:\", results2[\"metadatas\"][0][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
